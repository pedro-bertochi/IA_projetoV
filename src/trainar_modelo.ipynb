{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from core import criar_modelo, ParImageGenerator, create_tensorboard_callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos\n",
    "os.makedirs('../model', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "os.makedirs('../logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 🔁 Carregar todos os dados\n",
    "# ============================\n",
    "gen_temporario = ParImageGenerator('../image/treinamento', batch_size=1, augmentacao=False)\n",
    "dados = list(zip(gen_temporario.imagens, gen_temporario.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 🔁 K-Fold Cross-Validation\n",
    "# ============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌀 Treinando Fold 1...\n",
      "✅ Logs do TensorBoard serão salvos em: c:\\cod\\python\\AI\\logs\\fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\cod\\python\\AI\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "c:\\cod\\python\\AI\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - accuracy: 1.0000 - loss: 0.2316\n",
      "Epoch 1: val_accuracy improved from -inf to 0.80357, saving model to ../model/melhor_modelo_fold1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 47s/step - accuracy: 1.0000 - loss: 0.2316 - val_accuracy: 0.8036 - val_loss: 0.4999\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.8572 - loss: 0.4392\n",
      "📊 [Fold 1] Loss: 0.5000, Accuracy: 0.8036\n",
      "\n",
      "🌀 Treinando Fold 2...\n",
      "✅ Logs do TensorBoard serão salvos em: c:\\cod\\python\\AI\\logs\\fold_2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.2500 - loss: 0.8968\n",
      "Epoch 1: val_accuracy improved from -inf to 0.34821, saving model to ../model/melhor_modelo_fold2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 52s/step - accuracy: 0.2500 - loss: 0.8968 - val_accuracy: 0.3482 - val_loss: 0.7019\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3951 - loss: 0.7010\n",
      "📊 [Fold 2] Loss: 0.7022, Accuracy: 0.3393\n",
      "\n",
      "🌀 Treinando Fold 3...\n",
      "✅ Logs do TensorBoard serão salvos em: c:\\cod\\python\\AI\\logs\\fold_3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.5000 - loss: 0.7855\n",
      "Epoch 1: val_accuracy improved from -inf to 0.13393, saving model to ../model/melhor_modelo_fold3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 46s/step - accuracy: 0.5000 - loss: 0.7855 - val_accuracy: 0.1339 - val_loss: 1.1211\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.1080 - loss: 1.1438\n",
      "📊 [Fold 3] Loss: 1.1210, Accuracy: 0.1339\n",
      "\n",
      "🌀 Treinando Fold 4...\n",
      "✅ Logs do TensorBoard serão salvos em: c:\\cod\\python\\AI\\logs\\fold_4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.0000e+00 - loss: 1.2610\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11607, saving model to ../model/melhor_modelo_fold4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 40s/step - accuracy: 0.0000e+00 - loss: 1.2610 - val_accuracy: 0.1161 - val_loss: 0.9850\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.1536 - loss: 0.9624\n",
      "📊 [Fold 4] Loss: 0.9906, Accuracy: 0.1071\n",
      "\n",
      "🌀 Treinando Fold 5...\n",
      "✅ Logs do TensorBoard serão salvos em: c:\\cod\\python\\AI\\logs\\fold_5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19s/step - accuracy: 0.8750 - loss: 0.4643\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11607, saving model to ../model/melhor_modelo_fold5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 121s/step - accuracy: 0.8750 - loss: 0.4643 - val_accuracy: 0.1161 - val_loss: 1.6686\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.1136 - loss: 1.6731\n",
      "📊 [Fold 5] Loss: 1.6531, Accuracy: 0.1250\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "log_path_cv = '../results/cv_info.txt' # Renomeei para evitar confusão com logs de fase\n",
    "with open(log_path_cv, 'w') as log_file_cv: # Renomeei a variável\n",
    "\n",
    "    for train_idx, val_idx in kf.split(dados):\n",
    "        log_file_cv.write(f\"[Fold {fold}]\\n\")\n",
    "        log_file_cv.write(f\"Treino: {list(train_idx)}\\n\")\n",
    "        log_file_cv.write(f\"Validação: {list(val_idx)}\\n\\n\")\n",
    "\n",
    "        print(f\"\\n🌀 Treinando Fold {fold} - Fase 1 (Fine-tuning inicial)...\")\n",
    "\n",
    "        train_data = [dados[i] for i in train_idx]\n",
    "        val_data = [dados[i] for i in val_idx]\n",
    "\n",
    "        train_gen = ParImageGenerator(dados=train_data, batch_size=8, augmentacao=True)\n",
    "        val_gen = ParImageGenerator(dados=val_data, batch_size=8, augmentacao=False)\n",
    "\n",
    "        # --- FASE 1: Fine-tuning Inicial (todas as camadas treináveis) ---\n",
    "        modelo = criar_modelo() # Cria o modelo com todas as camadas da base treináveis por padrão\n",
    "\n",
    "        # Compila com learning rate para a Fase 1\n",
    "        modelo.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # Seu LR atual, bom para 1a fase de fine-tuning\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Callbacks para a Fase 1\n",
    "        checkpoint_ph1 = ModelCheckpoint(\n",
    "            filepath=f'../model/modelo_ph1_fold{fold}.h5', # Salva o modelo da Fase 1\n",
    "            monitor='val_loss', # Monitore val_loss para evitar overfitting\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        logger_ph1 = CSVLogger(f'../results/historico_fase1_fold{fold}.csv', append=False)\n",
    "        early_stopping_ph1 = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=7,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        tensorboard_callback_ph1 = create_tensorboard_callback(fold_name=f'fold_{fold}/fase_1') # Logs separados para fase 1\n",
    "\n",
    "        print(f\"--- Iniciando Fase 1 para Fold {fold} ---\")\n",
    "        modelo.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=15,\n",
    "            steps_per_epoch=len(train_gen),\n",
    "            callbacks=[checkpoint_ph1, logger_ph1, early_stopping_ph1, tensorboard_callback_ph1],\n",
    "            verbose=1\n",
    "        )\n",
    "        print(f\"--- Fase 1 para Fold {fold} concluída ---\")\n",
    "\n",
    "\n",
    "        # --- FASE 2: Fine-tuning Aprofundado (carrega melhor da Fase 1, ajusta camadas e LR) ---\n",
    "        print(f\"\\n🔧 Treinando Fold {fold} - Fase 2 (Fine-tuning aprofundado)...\")\n",
    "\n",
    "        # Carrega o MELHOR modelo da Fase 1 para continuar o treinamento\n",
    "        # É importante carregar o que foi salvo pelo checkpoint, pois o modelo 'modelo'\n",
    "        # após o fit pode não ser o melhor restaurado se early_stopping tiver parado cedo.\n",
    "        model_path_ph1 = f'../model/modelo_ph1_fold{fold}.h5'\n",
    "        if not os.path.exists(model_path_ph1):\n",
    "            print(f\"AVISO: Modelo da Fase 1 para Fold {fold} não encontrado em {model_path_ph1}. Pulando Fase 2 para este fold.\")\n",
    "            continue # Pula para o próximo fold se o modelo da fase 1 não foi salvo\n",
    "\n",
    "        modelo_ph2 = tf.keras.models.load_model(model_path_ph1)\n",
    "\n",
    "        # Ajusta as camadas treináveis para a Fase 2 (congelando as 100 primeiras)\n",
    "        if hasattr(modelo_ph2.layers[0], 'layers'): # Verifica se a primeira camada é o ResNet50\n",
    "            base_model_ph2 = modelo_ph2.layers[0]\n",
    "            base_model_ph2.trainable = True # Garante que a base está treinável antes de congelar partes\n",
    "            for layer in base_model_ph2.layers[:100]: # Congela as primeiras 100 camadas\n",
    "                layer.trainable = False\n",
    "        else:\n",
    "            print(f\"AVISO: A primeira camada do modelo do Fold {fold} não parece ser a base ResNet50. Fine-tuning da Fase 2 pode não funcionar como esperado.\")\n",
    "\n",
    "\n",
    "        # Compila com learning rate para a Fase 2 (mais baixo)\n",
    "        modelo_ph2.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # Seu LR mais baixo para fine-tuning aprofundado\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Callbacks para a Fase 2\n",
    "        checkpoint_ph2 = ModelCheckpoint(\n",
    "            filepath=f'../model/final_tuned_fold{fold}.h5', # Salva o modelo final do fold após a Fase 2\n",
    "            monitor='val_loss', # Mude para val_loss para salvar o menos overfitado\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        logger_ph2 = CSVLogger(f'../results/historico_fase2_fold{fold}.csv', append=False)\n",
    "        early_stopping_ph2 = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=7,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        tensorboard_callback_ph2 = create_tensorboard_callback(fold_name=f'fold_{fold}/fase_2') # Logs separados para fase 2\n",
    "\n",
    "        print(f\"--- Iniciando Fase 2 para Fold {fold} ---\")\n",
    "        modelo_ph2.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=10,\n",
    "            steps_per_epoch=len(train_gen),\n",
    "            callbacks=[checkpoint_ph2, logger_ph2, early_stopping_ph2, tensorboard_callback_ph2],\n",
    "            verbose=1\n",
    "        )\n",
    "        print(f\"--- Fase 2 para Fold {fold} concluída ---\")\n",
    "\n",
    "        # Avaliação final do modelo da Fase 2 para este fold\n",
    "        loss, accuracy = modelo_ph2.evaluate(val_gen, verbose=1)\n",
    "        print(f\"📊 [Fold {fold}] Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        log_file_cv.write(f\"Resultado Final Fold {fold}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\\n\\n\")\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 📈 Resultados finais\n",
    "# ============================\n",
    "media_acc = np.mean(accuracies)\n",
    "media_loss = np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cross-validation finalizada!\n",
      "📉 Média da Loss: 0.9934\n",
      "✅ Média da Accuracy: 0.3018\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ Cross-validation finalizada!\")\n",
    "print(f\"📉 Média da Loss: {media_loss:.4f}\")\n",
    "print(f\"✅ Média da Accuracy: {media_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resumo no log geral de CV\n",
    "with open(log_path_cv, 'a') as log_file_cv:\n",
    "    log_file_cv.write(\"=== Resultado Final CV ===\\n\")\n",
    "    log_file_cv.write(f\"Média da Loss: {media_loss:.4f}\\n\")\n",
    "    log_file_cv.write(f\"Média da Accuracy: {media_acc:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
